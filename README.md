# Mini BAGLS Dataset Project

Project Overview
This project involves working with a mini version of the Benchmark for Automatic Glottis Segmentation (BAGLS) dataset. The tasks covered include exploring the dataset and converting images from RGB to grayscale. This work helps to develop an understanding of handling and preprocessing image data for potential applications in computer vision tasks.

Dataset Description
The Benchmark for Automatic Glottis Segmentation (BAGLS) dataset is a comprehensive collection of laryngeal video frames, designed for training and benchmarking automatic glottis segmentation methods. For the purposes of this project, a smaller subset of the BAGLS dataset is used, providing an opportunity to perform basic image processing tasks efficiently.

Contents of the Mini BAGLS Dataset
Image Files: The dataset consists of a series of images in RGB format.
Annotations: Some images may be paired with annotations to assist in segmentation tasks, although annotation details may not be covered in this project.

Objectives
Familiarize with Image Datasets: Understand the structure and components of a real-world image dataset.
Image Processing: Perform an image transformation from RGB to grayscale using common programming libraries.
Setup and Requirements
Prerequisites
Python (3.x recommended)
Basic knowledge of image processing libraries such as OpenCV or PIL
Libraries
Make sure you have the following libraries installed:

numpy
opencv-python
matplotlib (for image visualization)

Workflow
Loading and Exploring the Dataset

1. Load the images from the mini BAGLS dataset.
2. Inspect the images to understand their structure and distribution.
3. Utilize image processing techniques to convert each RGB image into grayscale.
4. Visualize the original and grayscale images to compare and verify the transformation.

Contributing
If you would like to contribute to this project, feel free to fork the repository and submit a pull request. Suggestions and improvements are welcome!

Contact
For any questions or issues, please contact [vishakh.cheruparambath@fau.de]
